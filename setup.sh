#!/bin/bash

# ETL Pipeline Setup Script
echo "ğŸš€ Setting up ETL Pipeline with dbt, Snowflake, and Airflow..."

# Check if Python is installed
if ! command -v python3 &> /dev/null; then
    echo "âŒ Python 3 is required but not installed. Please install Python 3 first."
    exit 1
fi

# Check if pip is installed
if ! command -v pip3 &> /dev/null; then
    echo "âŒ pip3 is required but not installed. Please install pip3 first."
    exit 1
fi

echo "âœ… Python and pip are available"

# Install dbt and dependencies
echo "ğŸ“¦ Installing dbt and dependencies..."
pip3 install dbt-snowflake dbt-utils

# Install dbt packages
echo "ğŸ“¦ Installing dbt packages..."
dbt deps

# Check if Docker is installed (for Airflow)
if command -v docker &> /dev/null; then
    echo "âœ… Docker is available"
    
    # Check if docker-compose is installed
    if command -v docker-compose &> /dev/null; then
        echo "âœ… Docker Compose is available"
        echo "ğŸ³ You can run 'docker-compose up' to start Airflow"
    else
        echo "âš ï¸  Docker Compose not found. Install it to use the Docker setup."
    fi
else
    echo "âš ï¸  Docker not found. You can still use dbt locally."
    echo "ğŸ“¦ To install Airflow locally, run:"
    echo "   pip3 install apache-airflow"
    echo "   pip3 install -r requirements.txt"
fi

# Create necessary directories
echo "ğŸ“ Creating necessary directories..."
mkdir -p logs plugins

# Set permissions for Airflow
echo "ğŸ” Setting up permissions..."
export AIRFLOW_UID=50000

echo ""
echo "ğŸ‰ Setup complete!"
echo ""
echo "Next steps:"
echo "1. Update profiles.yml with your Snowflake credentials"
echo "2. Run 'dbt debug' to test your connection"
echo "3. Run 'dbt run' to execute the pipeline"
echo "4. For Airflow:"
echo "   - Copy .env.example to .env and update with your credentials"
echo "   - Run 'docker-compose up' to start Airflow"
echo "   - Access Airflow UI at http://localhost:8080"
echo ""
echo "ğŸ“š See README.md for detailed instructions"
